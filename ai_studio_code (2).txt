# Use an official Python runtime as a parent image
FROM python:3.10-slim

# Set the working directory in the container
WORKDIR /app

# Copy the dependencies file to the working directory
COPY requirements.txt .

# Install any needed packages specified in requirements.txt
# NOTE: llama-cpp-python can be tricky. For GPU support, you might need a different base image (e.g., nvidia/cuda)
# and to set CMAKE_ARGS. This is a basic CPU setup.
# For CPU with AVX2 support:
ENV CMAKE_ARGS="-DLLAMA_AVX2=ON"
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt

# Copy the rest of the application's code to the working directory
COPY . .

# Expose the port the app runs on
EXPOSE 8000

# Command to run the application
# We use 0.0.0.0 to make it accessible from outside the container
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]